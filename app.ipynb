{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68aa7f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\ragproject\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "# print(\"Hello world\")\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e96b356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ragproject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09321ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ee649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Groq LLM initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# STEP 0: Load environment variables\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Check if key exists\n",
    "if not GROQ_API_KEY:\n",
    "    raise ValueError(\"âŒ GROQ_API_KEY not found in .env file\")\n",
    "\n",
    "# Initialize Groq LLM\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(\"âœ… Groq LLM initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db544b26",
   "metadata": {},
   "source": [
    "### RAG has TWO phases â€” never mix them\n",
    "1ï¸âƒ£ Ingestion phase (slow, run ONCE)\n",
    "Load PDFs, Chunk, Embed, Save vector store\n",
    "\n",
    "2ï¸âƒ£ Query phase (fast, run MANY times)\n",
    "Load FAISS / pgvector & Ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a1e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“„ Loading postgresql-10-A4.pdf\n",
      "  postgresql-10-A4.pdf: 200 pages processed | chunks: 839\n",
      "  postgresql-10-A4.pdf: 400 pages processed | chunks: 1464\n",
      "  postgresql-10-A4.pdf: 600 pages processed | chunks: 2263\n",
      "  postgresql-10-A4.pdf: 800 pages processed | chunks: 3031\n",
      "  postgresql-10-A4.pdf: 1000 pages processed | chunks: 3591\n",
      "  postgresql-10-A4.pdf: 1200 pages processed | chunks: 4288\n",
      "  postgresql-10-A4.pdf: 1400 pages processed | chunks: 4798\n",
      "  postgresql-10-A4.pdf: 1600 pages processed | chunks: 5378\n",
      "  postgresql-10-A4.pdf: 1800 pages processed | chunks: 5873\n",
      "  postgresql-10-A4.pdf: 2000 pages processed | chunks: 6458\n",
      "  postgresql-10-A4.pdf: 2200 pages processed | chunks: 7187\n",
      "  postgresql-10-A4.pdf: 2400 pages processed | chunks: 7799\n",
      "\n",
      "ðŸ“„ Loading postgresql-11-A4.pdf\n",
      "  postgresql-11-A4.pdf: 200 pages processed | chunks: 9140\n",
      "  postgresql-11-A4.pdf: 400 pages processed | chunks: 9743\n",
      "  postgresql-11-A4.pdf: 600 pages processed | chunks: 10515\n",
      "  postgresql-11-A4.pdf: 800 pages processed | chunks: 11301\n",
      "  postgresql-11-A4.pdf: 1000 pages processed | chunks: 11880\n",
      "  postgresql-11-A4.pdf: 1200 pages processed | chunks: 12535\n",
      "  postgresql-11-A4.pdf: 1400 pages processed | chunks: 13089\n",
      "  postgresql-11-A4.pdf: 1600 pages processed | chunks: 13626\n",
      "  postgresql-11-A4.pdf: 1800 pages processed | chunks: 14146\n",
      "  postgresql-11-A4.pdf: 2000 pages processed | chunks: 14725\n",
      "  postgresql-11-A4.pdf: 2200 pages processed | chunks: 15298\n",
      "  postgresql-11-A4.pdf: 2400 pages processed | chunks: 15980\n",
      "  postgresql-11-A4.pdf: 2600 pages processed | chunks: 16606\n",
      "\n",
      "âœ… Finished chunking ALL PDFs\n",
      "ðŸ“¦ Total chunks created: 16995\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Data ingestion - loading PDFs and chunking them (RUN ONLY ONCE)\n",
    "# Load PDFs â†’ chunk â†’ produce `chunks`\n",
    "\n",
    "pdf_folder = r\"E:\\ragproject\\Pdf_files\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for file in os.listdir(pdf_folder):\n",
    "    if not file.lower().endswith(\".pdf\"):\n",
    "        continue\n",
    "\n",
    "    full_path = os.path.join(pdf_folder, file)\n",
    "    print(f\"\\nðŸ“„ Loading {file}\")\n",
    "\n",
    "    loader = PyMuPDFLoader(full_path)\n",
    "\n",
    "    for i, doc in enumerate(loader.lazy_load(), start=1):\n",
    "        # keep metadata\n",
    "        doc.metadata[\"source\"] = file\n",
    "        doc.metadata[\"page\"] = i\n",
    "\n",
    "        chunks.extend(splitter.split_documents([doc]))\n",
    "\n",
    "        if i % 200 == 0:\n",
    "            print(f\"  {file}: {i} pages processed | chunks: {len(chunks)}\")\n",
    "\n",
    "print(f\"\\nâœ… Finished chunking ALL PDFs\")\n",
    "print(f\"ðŸ“¦ Total chunks created: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b68991b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ragproject\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ajith\\.cache\\huggingface\\hub\\models--BAAI--bge-small-en-v1.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded 512 / 16995\n",
      "Embedded 1024 / 16995\n",
      "Embedded 1536 / 16995\n",
      "Embedded 2048 / 16995\n",
      "Embedded 2560 / 16995\n",
      "Embedded 3072 / 16995\n",
      "Embedded 3584 / 16995\n",
      "Embedded 4096 / 16995\n",
      "Embedded 4608 / 16995\n",
      "Embedded 5120 / 16995\n",
      "Embedded 5632 / 16995\n",
      "Embedded 6144 / 16995\n",
      "Embedded 6656 / 16995\n",
      "Embedded 7168 / 16995\n",
      "Embedded 7680 / 16995\n",
      "Embedded 8192 / 16995\n",
      "Embedded 8704 / 16995\n",
      "Embedded 9216 / 16995\n",
      "Embedded 9728 / 16995\n",
      "Embedded 10240 / 16995\n",
      "Embedded 10752 / 16995\n",
      "Embedded 11264 / 16995\n",
      "Embedded 11776 / 16995\n",
      "Embedded 12288 / 16995\n",
      "Embedded 12800 / 16995\n",
      "Embedded 13312 / 16995\n",
      "Embedded 13824 / 16995\n",
      "Embedded 14336 / 16995\n",
      "Embedded 14848 / 16995\n",
      "Embedded 15360 / 16995\n",
      "Embedded 15872 / 16995\n",
      "Embedded 16384 / 16995\n",
      "Embedded 16896 / 16995\n",
      "Embedded 16995 / 16995\n",
      "âœ… FAISS index saved successfully\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: CREATE EMBEDDINGS (RUN ONLY ONCE)\n",
    "# chunks â†’ FAISS â†’ save_local()\n",
    "# Choose the right embedding model (fast + good)\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "\n",
    "# Create FAISS index in BATCHES (IMPORTANT)\n",
    "\n",
    "batch_size = 512\n",
    "vectorstore = None\n",
    "total = len(chunks)\n",
    "\n",
    "for i in range(0, total, batch_size):\n",
    "    batch = chunks[i:i + batch_size]\n",
    "\n",
    "    if vectorstore is None:\n",
    "        vectorstore = FAISS.from_documents(batch, embeddings)\n",
    "    else:\n",
    "        vectorstore.add_documents(batch)\n",
    "\n",
    "    print(f\"Embedded {min(i + batch_size, total)} / {total}\")\n",
    "\n",
    "vectorstore.save_local(\"faiss_pg_10_11\")\n",
    "\n",
    "print(\"âœ… FAISS index saved successfully\")\n",
    "\n",
    "# This is the hardest and slowest step in RAG \n",
    "# (And a new folder: `faiss_pg_10_11` should be created in your project directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f80351d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Embedding Dimensions \n",
    "print(\"FAISS dimension:\", vectorstore.index.d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f08065d",
   "metadata": {},
   "source": [
    "### Restart Kernel\n",
    "This frees RAM and avoids accidental re-runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347cdc1a",
   "metadata": {},
   "source": [
    "### STAGE 1: Single-stage RAG (Retrieve â†’ Generate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a04cb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FAISS loaded, ready for queries\n"
     ]
    }
   ],
   "source": [
    "# ONLY load the FAISS index (This is now default starting point for RAG)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"faiss_pg_10_11\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "print(\"âœ… FAISS loaded, ready for queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f1ec91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 documents for query: 'What is the purpose of this project?'\n",
      "\n",
      "Document 1:\n",
      "CREATE PUBLICATION ................................................................................. 1480\n",
      "CREATE ROLE ..................................................................................\n",
      "\n",
      "Document 2:\n",
      "The target schema is determined by the schema parameter in the control file if that is given, other-\n",
      "wise by the SCHEMA option of CREATE EXTENSION if that is given, otherwise the current default\n",
      "objec\n",
      "\n",
      "Document 3:\n",
      "CREATE POLICY ................................................................................................... 1474\n",
      "CREATE PUBLICATION ..............................................................\n"
     ]
    }
   ],
   "source": [
    "# Quick Retrieval Test before we connect the LLM (to make sure everything works)\n",
    "\n",
    "query = \"What is the purpose of this project?\"\n",
    "docs = vectorstore.similarity_search(query, k=3)\n",
    "print(f\"Found {len(docs)} documents for query: '{query}'\")\n",
    "\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"\\nDocument {i}:\")\n",
    "    print(doc.page_content[:200])  # Show first 200 characters of each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7065aa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: postgresql-10-A4.pdf | PAGE: 758\n",
      "Chapter 31. Logical Replication\n",
      "Logical replication is a method of replicating data objects and their changes, based upon their replica-\n",
      "tion identity (usually a primary key). We use the term logical in contrast to physical replication, which\n",
      "uses exact block addresses and byte-by-byte replication. \n",
      "--------------------------------------------------\n",
      "SOURCE: postgresql-11-A4.pdf | PAGE: 799\n",
      "Chapter 31. Logical Replication\n",
      "Logical replication is a method of replicating data objects and their changes, based upon their replica-\n",
      "tion identity (usually a primary key). We use the term logical in contrast to physical replication, which\n",
      "uses exact block addresses and byte-by-byte replication. \n",
      "--------------------------------------------------\n",
      "SOURCE: postgresql-10-A4.pdf | PAGE: 680\n",
      "can be quickly made the new master database server. This can be synchronous or asynchronous\n",
      "and can only be done for the entire database server.\n",
      "A standby server can be implemented using file-based log shipping (Section 26.2) or streaming\n",
      "replication (see Section 26.2.5), or a combination of both. F\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "docs = vectorstore.similarity_search(\n",
    "    \"What is logical replication in PostgreSQL 11?\",\n",
    "    k=3\n",
    ")\n",
    "\n",
    "for d in docs:\n",
    "    print(\"SOURCE:\", d.metadata.get(\"source\"), \"| PAGE:\", d.metadata.get(\"page\"))\n",
    "    print(d.page_content[:300])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f243961a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logical replication is a method of replicating data objects and their changes, based upon their replication identity (usually a primary key). It uses a publish and subscribe model with one or more subscribers subscribing to one or more publications on a publisher node.\n"
     ]
    }
   ],
   "source": [
    "# Answer Generation\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0,\n",
    "    groq_api_key=GROQ_API_KEY\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "context = format_docs(docs)\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are a PostgreSQL expert.\n",
    "\n",
    "Answer ONLY using the context below.\n",
    "If the answer is not in the context, say you don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "What is logical replication in PostgreSQL 11?\n",
    "\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "print(response.content)\n",
    "\n",
    "# If this looks good â†’ RAG backbone is solid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ca4bf1",
   "metadata": {},
   "source": [
    "Till now we can call this as Vanilla RAG / Single-pass Retrieval-Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4495709",
   "metadata": {},
   "source": [
    "### ## Manual self-corrective RAG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e313906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Score: 1\n",
      "\n",
      "âœ… Final Decision: ACCEPT ANSWER\n",
      "\n",
      "ðŸ“Œ Final Answer:\n",
      "\n",
      "Logical replication is a method of replicating data objects and their changes, based upon their replication identity (usually a primary key). It uses a publish and subscribe model with one or more subscribers subscribing to one or more publications on a publisher node.\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE ANSWER + FINAL DECISION\n",
    "\n",
    "eval_prompt = f\"\"\"\n",
    "You are an evaluator.\n",
    "\n",
    "Question:\n",
    "What is logical replication in PostgreSQL 11?\n",
    "\n",
    "Answer:\n",
    "{response.content}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Rate the answer from 0 to 1 based on:\n",
    "- factual correctness\n",
    "- completeness\n",
    "- grounding in context\n",
    "\n",
    "Respond ONLY with a number.\n",
    "\"\"\"\n",
    "\n",
    "eval_response = llm.invoke(eval_prompt)\n",
    "print(\"Evaluation Score:\", eval_response.content)\n",
    "\n",
    "\n",
    "# FINAL THRESHOLD STRATEGY\n",
    "\n",
    "try:\n",
    "    score = float(eval_response.content.strip())\n",
    "except ValueError:\n",
    "    score = 0.0\n",
    "\n",
    "if score >= 0.7:\n",
    "    print(\"\\nâœ… Final Decision: ACCEPT ANSWER\")\n",
    "    final_answer = response.content\n",
    "\n",
    "elif 0.3 <= score < 0.7:\n",
    "    print(\"\\nâš ï¸ Final Decision: NEEDS REFINEMENT\")\n",
    "    final_answer = (\n",
    "        \"The answer may be partially supported by the documents. \"\n",
    "        \"Additional retrieval or clarification is recommended.\"\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\"\\nâŒ Final Decision: NOT IN DOCUMENTATION\")\n",
    "    final_answer = \"I don't have enough information from the documents.\"\n",
    "\n",
    "print(\"\\nðŸ“Œ Final Answer:\\n\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726cff54",
   "metadata": {},
   "source": [
    "### STAGE 2: Multistage RAG / Self-Corrective RAG\n",
    "The system reasons, retrieves, evaluates, and possibly retrieves again before answering\n",
    "\n",
    "The LLM critiques its own answer and fixes it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873d3f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag_chain = (\n",
    "#     {\n",
    "#         \"context\": retriever | format_docs,\n",
    "#         \"question\": RunnablePassthrough(),\n",
    "#     }\n",
    "#     | prompt\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "\n",
    "# print(\"ðŸš€ RAG System Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b347f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class RAGState(TypedDict):\n",
    "    question: str\n",
    "    docs: List[Document]\n",
    "    answer: str\n",
    "    score: float\n",
    "    retry_count: int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33201067",
   "metadata": {},
   "source": [
    "### ðŸ§© STEP 1 â€” Define LangGraph Nodes\n",
    "Each node = one logical stage (same as manual code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3414219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 1: Retrieve\n",
    "\n",
    "def retrieve(state: RAGState):\n",
    "    docs = vectorstore.similarity_search(state[\"question\"], k=4)\n",
    "    return {\"docs\": docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6d5bf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 2: Generate Answer\n",
    "\n",
    "def generate(state: RAGState):\n",
    "    context = format_docs(state[\"docs\"])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a PostgreSQL expert.\n",
    "\n",
    "Answer ONLY using the context below.\n",
    "If the answer is not in the context, say you don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{state[\"question\"]}\n",
    "\"\"\"\n",
    "\n",
    "    answer = llm.invoke(prompt).content\n",
    "    return {\"answer\": answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8c531a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 3: Evaluate Answer (LLM-as-Critic)\n",
    "\n",
    "import re\n",
    "\n",
    "def evaluate(state: RAGState):\n",
    "    context = format_docs(state[\"docs\"])\n",
    "\n",
    "    eval_prompt = f\"\"\"\n",
    "You are an evaluator.\n",
    "\n",
    "Question:\n",
    "{state[\"question\"]}\n",
    "\n",
    "Answer:\n",
    "{state[\"answer\"]}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Rate the answer from 0 to 1 based on:\n",
    "- factual correctness\n",
    "- completeness\n",
    "- grounding in context\n",
    "\n",
    "Respond ONLY with a number.\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.invoke(eval_prompt).content.strip()\n",
    "    match = re.search(r\"\\d*\\.?\\d+\", response)\n",
    "    score = float(match.group()) if match else 0.0\n",
    "\n",
    "    return {\"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ecc41b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 4: Refine Query (Retry Once)\n",
    "\n",
    "def refine(state: RAGState):\n",
    "    refine_prompt = f\"\"\"\n",
    "Rewrite this question to retrieve better PostgreSQL documentation.\n",
    "\n",
    "Original question:\n",
    "{state[\"question\"]}\n",
    "\"\"\"\n",
    "\n",
    "    new_question = llm.invoke(refine_prompt).content.strip()\n",
    "\n",
    "    return {\n",
    "        \"question\": new_question,\n",
    "        \"retry_count\": state[\"retry_count\"] + 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1906b276",
   "metadata": {},
   "source": [
    "### ðŸ§  STEP 2 â€” Decision Logic (Your Threshold Strategy)\n",
    "This is exactly what you already designed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eabf513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "\n",
    "def decide(state: RAGState):\n",
    "    score = state[\"score\"]\n",
    "    retry_count = state[\"retry_count\"]\n",
    "\n",
    "    if score >= 0.7:\n",
    "        return \"accept\"\n",
    "\n",
    "    if 0.3 <= score < 0.7 and retry_count < 1:\n",
    "        return \"refine\"\n",
    "\n",
    "    return \"reject\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ef3665",
   "metadata": {},
   "source": [
    "### ðŸ§± STEP 3 â€” Build the LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6617b167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "\n",
    "graph = StateGraph(RAGState)\n",
    "\n",
    "graph.add_node(\"retrieve\", retrieve)\n",
    "graph.add_node(\"generate\", generate)\n",
    "graph.add_node(\"evaluate\", evaluate)\n",
    "graph.add_node(\"refine\", refine)\n",
    "\n",
    "graph.set_entry_point(\"retrieve\")\n",
    "\n",
    "graph.add_edge(\"retrieve\", \"generate\")\n",
    "graph.add_edge(\"generate\", \"evaluate\")\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"evaluate\",\n",
    "    decide,\n",
    "    {\n",
    "        \"accept\": END,\n",
    "        \"refine\": \"refine\",\n",
    "        \"reject\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "graph.add_edge(\"refine\", \"retrieve\")\n",
    "\n",
    "rag_graph = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c1353",
   "metadata": {},
   "source": [
    "### ðŸš€ STEP 4 â€” Run the LangGraph RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7dc00a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL ANSWER: Logical replication in PostgreSQL differs from physical replication in that it uses exact block addresses and byte-by-byte replication, whereas logical replication is based upon their replication identity (usually a primary key).\n",
      "FINAL SCORE: 0.9\n"
     ]
    }
   ],
   "source": [
    "result = rag_graph.invoke({\n",
    "    \"question\": \"How does logical replication differ from physical replication in PostgreSQL 11?\",\n",
    "    \"retry_count\": 0\n",
    "})\n",
    "\n",
    "print(\"FINAL ANSWER:\", result.get(\"answer\"))\n",
    "print(\"FINAL SCORE:\", result.get(\"score\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0df461",
   "metadata": {},
   "source": [
    "Basic RAG\tâœ…\n",
    "\n",
    "Manual self-corrective RAG\tâœ…\n",
    "\n",
    "LangGraph self-corrective RAG\tâœ…\n",
    "\n",
    "Confidence-based routing\tâœ…\n",
    "\n",
    "Retry with refinement\tâœ…\n",
    "\n",
    "Hallucination rejection\tâœ…\n",
    "\n",
    "Production-grade control flow\tâœ…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be126734",
   "metadata": {},
   "source": [
    "I want to do \n",
    "1. Scale this LangGraph to all PDFs - Time \n",
    "2. Wrap this graph in FastAPI\n",
    "3. Move FAISS â†’ PostgreSQL pgvector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
